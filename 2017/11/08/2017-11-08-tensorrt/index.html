<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  <title>TensorRT | Aelous</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="什么是TensorRTTensorRT™是NVIDIA出品的一款高性能的深度学习推理优化器和运行库，可为深度学习应用提供低延迟，高吞吐量推理。 TensorRT可用于快速优化，验证和部署经过训练的神经网络 TensorRT做了哪些优化 通过将精度量化到INT8，显着提高FP32全精度模型的inference性能，同时最大限度地降低精度损失 通过将连续的节点组合到单个节点中来优化GPU利用率并优化了">
<meta property="og:type" content="article">
<meta property="og:title" content="TensorRT">
<meta property="og:url" content="http://yoursite.com/2017/11/08/2017-11-08-tensorrt/index.html">
<meta property="og:site_name" content="Aelous">
<meta property="og:description" content="什么是TensorRTTensorRT™是NVIDIA出品的一款高性能的深度学习推理优化器和运行库，可为深度学习应用提供低延迟，高吞吐量推理。 TensorRT可用于快速优化，验证和部署经过训练的神经网络 TensorRT做了哪些优化 通过将精度量化到INT8，显着提高FP32全精度模型的inference性能，同时最大限度地降低精度损失 通过将连续的节点组合到单个节点中来优化GPU利用率并优化了">
<meta property="og:updated_time" content="2017-11-08T09:52:09.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="TensorRT">
<meta name="twitter:description" content="什么是TensorRTTensorRT™是NVIDIA出品的一款高性能的深度学习推理优化器和运行库，可为深度学习应用提供低延迟，高吞吐量推理。 TensorRT可用于快速优化，验证和部署经过训练的神经网络 TensorRT做了哪些优化 通过将精度量化到INT8，显着提高FP32全精度模型的inference性能，同时最大限度地降低精度损失 通过将连续的节点组合到单个节点中来优化GPU利用率并优化了">
  
    <link rel="alternate" href="/atom.xml" title="Aelous" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/typing.css">
</head>

  
    
      <body>
    
  
      <div id="container" class="container">
        <article id="post_layout-2017-11-08-tensorrt" class="article article-type-post_layout" itemscope itemprop="blogPost">
  <header id="header" class="header">
  <nav id="main-nav" class="main-nav">
    
      <a class="main-nav-link" href="/">Home</a>
    
      <a class="main-nav-link" href="/archives">Archives</a>
    
      <a class="main-nav-link" href="/about">About</a>
    
  </nav>
  <nav id="sub-nav">
    
      <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
    
  </nav>
</header>

  <hr/>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      TensorRT
    </h1>
  

      </header>
    
    <div class="article-entry typo" itemprop="articleBody">
      
        <h1 id="什么是TensorRT"><a href="#什么是TensorRT" class="headerlink" title="什么是TensorRT"></a>什么是TensorRT</h1><p>TensorRT™是NVIDIA出品的一款高性能的深度学习推理优化器和运行库，可为深度学习应用提供低延迟，高吞吐量推理。 TensorRT可用于快速优化，验证和部署经过训练的神经网络</p>
<h1 id="TensorRT做了哪些优化"><a href="#TensorRT做了哪些优化" class="headerlink" title="TensorRT做了哪些优化"></a>TensorRT做了哪些优化</h1><ul>
<li>通过将精度量化到INT8，显着提高FP32全精度模型的inference性能，同时最大限度地降低精度损失</li>
<li>通过将连续的节点组合到单个节点中来优化GPU利用率并优化了单个内核执行时的存储器存储和带宽</li>
<li>通过为GPU平台选择最佳数据层和最佳并行算法来优化执行时间</li>
<li>只为每个使用时的tensor分配内存从而减少内存占用量并提升内存重用率</li>
<li>将串行的输入流并行执行从而减少运行时间 </li>
</ul>
<h1 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h1><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p>download： <a href="https://developer.nvidia.com/nvidia-tensorrt3rc-download" target="_blank" rel="external">https://developer.nvidia.com/nvidia-tensorrt3rc-download</a><br>install：</p>
<ol>
<li>Install the following dependencies, if not already present:<br>‣ Install the CUDA Toolkit v8.0 or 9.0<br>‣ cuDNN 7.0.1<br>‣ Python 2 or Python 3</li>
<li>Choose where you want to install. This tar file will install everything into a directory called TensorRT-3.0.0. This directory will have sub-directories like lib, include, data, etc…</li>
<li>Unpack the tar file.<br>$ tar xzvf TensorRT-3.0.0.Ubuntu-16.04.3.cuda-9.0.tar.gz<br>$ ls TensorRT-3.0.0<br> bin data doc include lib python samples targets TensorRT-ReleaseNotes.pdf uff</li>
<li>Add the absolute path of TensorRT lib to the environment variable $LD_LIBRARY_PATH.<br>$ export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:</li>
<li>Install the Python TensorRT package.<br>$ cd TensorRT-3.0.0/python<br>If using Python 2.7:<br>$ sudo pip2 install tensorrt-3.0.0-cp27-cp27mu-linux_x86_64.whl<br>If using Python 3.5:<br>$ sudo pip3 install tensorrt-3.0.0-cp35-cp35m-linux_x86_64.whl<br>In either case:<br>$ which tensorrt<br>/usr/local/bin/tensorrt</li>
<li>Install the Python UFF package.<br>$ cd TensorRT-3.0.0/python<br>If using Python 2.7:<br>$ sudo pip2 install uff-0.1.0rc0-py2.py3-none-any.whl<br>If using Python 3.5:<br>$ sudo pip3 install uff-0.1.0rc0-py2.py3-none-any.whl<br>In either case:<br>$ which convert-to-uff<br>/usr/local/bin/convert-to-uff</li>
<li>Copy the mnist/lenet5.uff file from data/ to the python/data/ directory.<br> sudo cp TensorRT-3.0.0/data/mnist/lenet5.uff TensorRT-3.0.0/python/data/mnist/lenet5.uff</li>
<li>Verify the installation:<br> a) Ensure that the installed files are located in the correct directories. For example, run the tree -d command to check whether all supported installed files are in place in the lib, include, data, etc… directories.<br> b) Build and run one of the shipped samples, for example, sampleMNIST in the installed directory. The sample should be</li>
</ol>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="基于caffe"><a href="#基于caffe" class="headerlink" title="基于caffe"></a>基于caffe</h3><p>在使用caffe进行inference时,只需将这个步骤<code>out = net.forward()</code>改为如下函数即可，相关参数需要改下<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div></pre></td><td class="code"><pre><div class="line">import tensorrt as trt</div><div class="line">import pycuda.driver as cuda</div><div class="line">import pycuda.autoinit</div><div class="line">import numpy as np</div><div class="line">from random import randint</div><div class="line">from PIL import Image</div><div class="line">from tensorrt.parsers import caffeparser</div><div class="line"> </div><div class="line">MODEL_PROTOTXT = &apos;deploy.prototxt&apos;                     # deploy文件</div><div class="line">CAFFE_MODEL = &apos;caffenet_train_iter_75000.caffemodel&apos;   # caffe模型</div><div class="line">IMAGE_MEAN = &apos;image_mean.binaryproto&apos;                  # 数据均值文件</div><div class="line">INPUT_LAYERS = [&apos;data&apos;]                                # 输入数据层名称</div><div class="line">OUTPUT_LAYERS = [&apos;prob&apos;]                               # 输出数据层名称</div><div class="line">INPUT_H = 227                                          # 输入图片高</div><div class="line">INPUT_W = 227                                          # 输出图片高</div><div class="line">OUTPUT_SIZE = 3                                        # 输出数据大小</div><div class="line"> </div><div class="line"> </div><div class="line">def use_trt(img):   </div><div class="line">        &quot;&quot;&quot;</div><div class="line">        Use tensorRT to replace caffe&apos;s process: net.forward()</div><div class="line"> </div><div class="line">        Parameters</div><div class="line">        ----------</div><div class="line">        img: 输入数据。在使用caffe框架进行inference时，给Net输入数据 net.blobs[&apos;data&apos;].data[i, :, :, :] = transformed_image， 将transformed_image 作为此函数的输入即可</div><div class="line"> </div><div class="line">        Returns</div><div class="line">        -------</div><div class="line">        output: 输出数据。 在使用caffe框架进行inference时，使用net.forward()输出的数据与次输出一致</div><div class="line"> </div><div class="line">        &quot;&quot;&quot;</div><div class="line"> </div><div class="line"> </div><div class="line">    G_LOGGER = trt.infer.ConsoleLogger(trt.infer.LogSeverity.ERROR)   </div><div class="line"> </div><div class="line">    # create engine，Parameters： a logger, a path to the model prototxt, the model file, the max batch size, the max workspace size, the output layers and the data type of the weights</div><div class="line">    engine = trt.utils.caffe_to_trt_engine(G_LOGGER,MODEL_PROTOTXT,CAFFE_MODEL,1,1 &lt;&lt; 20,OUTPUT_LAYERS,trt.infer.DataType.FLOAT)</div><div class="line"> </div><div class="line"> </div><div class="line">    #create runtime and context</div><div class="line">    runtime = trt.infer.create_infer_runtime(G_LOGGER)</div><div class="line">    context = engine.create_execution_context()</div><div class="line"> </div><div class="line"> </div><div class="line">    #create output array to receive data</div><div class="line">    output = np.empty(OUTPUT_SIZE, dtype = np.float32)</div><div class="line"> </div><div class="line">    # allocate memory on GPU with pyCUDA, and register them with the engine</div><div class="line">    d_input = cuda.mem_alloc(1 * img.size * img.dtype.itemsize)</div><div class="line">    d_output = cuda.mem_alloc(1 * output.size * output.dtype.itemsize)</div><div class="line">    bindings = [int(d_input), int(d_output)]</div><div class="line">    stream = cuda.Stream()</div><div class="line">  </div><div class="line">    #transfer input data to device</div><div class="line">    cuda.memcpy_htod_async(d_input, img, stream)</div><div class="line"> </div><div class="line">    #execute model</div><div class="line">    context.enqueue(1, bindings, stream.handle, None)</div><div class="line"> </div><div class="line">    #transfer predictions back</div><div class="line">    cuda.memcpy_dtoh_async(output, d_output, stream)</div><div class="line"> </div><div class="line">    #synchronize threads</div><div class="line">    stream.synchronize()</div><div class="line"> </div><div class="line">    context.destroy()</div><div class="line">    engine.destroy()</div><div class="line">    runtime.destroy()</div><div class="line"> </div><div class="line">    return output</div></pre></td></tr></table></figure></p>
<h1 id="基准测试"><a href="#基准测试" class="headerlink" title="基准测试"></a>基准测试</h1><p>据官方描述，TensorRT在caffe框架上，根据batch size的不同，其inference速度能有2-5倍的提升。而如果使用 INT8 精度会比使用FP32精度的速度提升2-3倍</p>

      
    </div>
    <footer class="article-footer">
      <ul class="article-meta">
        <li>
          <span class="label">Published Date:</span>
          <a href="/2017/11/08/2017-11-08-tensorrt/" class="article-date">
  <time datetime="2017-11-07T16:00:00.000Z" itemprop="datePublished">2017-11-08</time>
</a>

        </li>
        
        
        <hr/>
      </ul>
    </footer>
  </div>
  
    
<nav id="article-nav" class="article-nav">
  
    <a href="/2017/11/09/2017-11-09-decorator/" id="article-nav-newer" class="article-nav-link-wrap newer">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          装饰器
        
      </div>
    </a>
  
  
    <a href="/2017/04/29/2017-04-29-tmux/" id="article-nav-older" class="article-nav-link-wrap older">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">TMUX</div>
    </a>
  
</nav>


  
</article>




      </div>
      
    <footer id="footer" class="post-footer footer">
      <hr/>
      <div id="footerContent" class="footer-content">
        <p>ipsum dolor sit amet, <strong>consectetur adipiscing elit.</strong> Fusce eget urna vitae velit <em>eleifend interdum at ac nisi. In nec ligula lacus. Cum sociis natoque</em> penatibus et magnis dis parturient montes, nascetur ridiculus mus. Sed eu cursus erat, ut dapibus quam. Post</p>


      </div>
    </footer>

      

<script src="//cdn.bootcss.com/jquery/2.2.4/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/typing.js"></script>
<!--[if lt IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7/html5shiv.min.js"></script><![endif]-->







    </div>
  </body>
</html>
